# 高性能PC導入によるAI検証

## PCスペック
- CPU: Ryzen AI 7 350
- GPU: NVIDIA GeForce RTX 5070
- RAM: 64GB DDR5

## 検証概要
本PCを用いて、ローカル環境での生成AI活用を想定した検証を実施した。

- **目的**
  - 社内情報をクラウドに出さずに利用できるローカルAI基盤の実現性確認
  - チャット支援、要約、コード補完など、日常業務レベルでの実用性評価
  - モデルサイズ別のパフォーマンスとリソース利用状況の把握
- **前提**
  - 主な実行環境は LM Studio（ローカルLLMランタイム）
  - モデルは Gemma3 と gpt-oss-20B を LM Studio 上で稼働させて評価

## 検証環境・ツール
- **OS / ハードウェア**
  - 上記 PC スペックを使用
  - GPU 利用を前提とした GGUF 形式モデルを想定
- **主なツール**
  - LM Studio
    - Model download & management
    - Local inference
    - OpenAI compatible API server

### LM Studio 設定（例）
- **Model**
  - Gemma3 系モデル: 軽量・高速応答向け
  - gpt-oss-20B: 高品質回答・日本語長文向け
- **Quantization**
  - Q4〜Q5 程度を中心に検証（品質とメモリ消費のバランス重視）
- **Context length**
  - 4K〜16K tokens をタスクに応じて切り替え
- **GPU 使用設定**
  - GPU Layers を可能な範囲で最大に設定し、レイテンシ削減を優先

## 検証シナリオ
- **業務チャット支援**
  - 社内ドキュメントを想定したテキストを読み込ませ、Q&A を実施
  - 丁寧な文体での回答生成、要点抽出の精度を確認
- **要約・議事録生成**
  - 3,000〜10,000 文字程度の日本語テキストを入力し、箇条書き要約やアクションアイテム抽出を評価
- **コード支援**
  - 既存コードのリファクタ案の提案
  - バグのありそうな箇所の指摘と修正案生成
  - 単体テストコードの自動生成
- **データ解釈**
  - ログやメトリクスの説明、原因候補の列挙など、仮説出し支援

## 結果サマリ（定性的評価）
- **Gemma3 系モデル（軽量プロファイル）**
  - 日常的なチャット・要約・軽めのコードレビュー用途では十分な品質
  - 応答速度も良好で、対話体験としてストレスは少ない
  - GPU メモリ使用量に余裕があり、他アプリとの併用も現実的
- **gpt-oss-20B モデル（高品質プロファイル）**
  - 日本語長文の理解や、複雑なコードリファクタ提案で Gemma3 より精度向上が見られる
  - 推論時間は Gemma3 比で増加するが、本PCスペックであれば実用範囲内
- **総評**
  - 本PCクラスであれば、ローカルのみで「実務で使えるレベル」の生成AI環境を構築可能
  - 機密情報を外部に出さずに、ある程度の高度なアシスタント機能を実現できる見込み

## LM Studio の活用方法
- **チャットUIでの利用**
  - LM Studio の Chat タブから直接プロンプトを入力し、応答品質やレイテンシを確認
  - System prompt を調整し、社内ルールや文体を事前に指示することで、望ましい応答傾向を誘導
- **API Server 機能の利用**
  - LM Studio から OpenAI 互換のエンドポイントを起動
    - Example: `http://localhost:1234/v1/chat/completions`
  - 各種クライアント（例: VS Code 拡張、社内ツール）から HTTP 経由でローカルモデルを呼び出し
- **モデルプロファイルの使い分け**
  - 軽量プロファイル（Gemma3 / 低精度量子化）: 高速応答・ブレインストーミング用途
  - 高品質プロファイル（gpt-oss-20B / 高精度量子化）: 重要文書のドラフト作成やコードレビュー用途

## リソース利用と運用上の注意
- **GPU / メモリ**
  - 複数モデルを同時に常駐させると VRAM / RAM を圧迫するため、同時稼働数を制御する運用が望ましい
  - 大規模モデル利用時は、他の GPU 負荷の高い処理（ゲーム、動画編集等）との同時実行は避ける
- **ストレージ**
  - モデルファイルは 1 つあたり数 GB〜数十 GB に達するため、保存先ディスク容量を事前に確保
- **セキュリティ / プライバシー**
  - 推論処理はローカル完結であり、原則として社外への情報送信は発生しない
  - ただし、モデル配布元やアップデート時の通信経路は社内ポリシーに準拠して管理する

## 今後の検討事項
- **RAG（Retrieval-Augmented Generation）との連携**
  - 社内ドキュメントやナレッジベースをローカルでインデックス化し、LM Studio と外部ツールを組み合わせて検索拡張を行う構成の検証
- **複数ユーザー利用**
  - 同一マシン上の LM Studio API Server を、社内ネットワーク内の複数ユーザーから共用する運用の可能性評価
- **モデルラインナップの拡充**
  - 日本語特化モデル、コード特化モデルなどを追加し、用途別に最適なモデル選択ができるようにする
- **監視・ログ**
  - 利用状況（リクエスト数、レイテンシ、エラー率など）を計測し、将来的なスケールアウトや追加GPU導入の判断材料とする

