# 高性能PC導入によるAI検証

## PCスペック
- CPU: Ryzen AI 7 350
- GPU: NVIDIA GeForce RTX 5070
- RAM: 64GB DDR5

## 検証概要
本PCを用いて、ローカル環境での生成AI活用を想定した検証を実施した。

- **目的**
  - 社内情報をクラウドに出さずに利用できるローカルAI基盤の実現性確認
  - チャット支援、要約、コード補完など、日常業務レベルでの実用性評価
  - モデルサイズ別のパフォーマンスとリソース利用状況の把握
- **前提**
  - 主な実行環境は LM Studio（ローカルLLMランタイム）
  - モデルは Gemma3 と gpt-oss-20B を LM Studio 上で稼働させて評価

## 検証環境・ツール
- **OS / ハードウェア**
  - 上記 PC スペックを使用
  - GPU 利用を前提とした GGUF 形式モデルを想定
- **主なツール**
  - LM Studio
    - Model download & management
    - Local inference
    - OpenAI compatible API server

### LM Studio 設定（例）
- **Model**
  - Gemma3 系モデル: 軽量・高速応答向け
  - gpt-oss-20B: 高品質回答・日本語長文向け

## 検証シナリオ
- **メール本文生成**
  - 宛先・目的・要点を入力し、ビジネスメール文面を自動生成
  - 敬体・丁寧語の適切さ、署名や挨拶文の自然さを評価
- **メール要約**
  - 長文メールや複数往復のスレッドを入力し、要点を箇条書きで抽出
  - アクションアイテムや期限の抽出精度を確認
- **日本語翻訳**
  - 英語メールを日本語に翻訳し、ビジネス文脈での自然さを評価
  - 専門用語や社内用語の訳出精度を確認
- **API経由での検証**
  - LM Studio の OpenAI 互換 API (`http://localhost:1234/v1/chat/completions`) を使用
  - プロンプトテンプレート（`Mail/Prompt/default.txt`）を活用した一貫性のある出力形式の検証
  - JSON形式での構造化出力（件名・本文の分離）の安定性を評価
 
## 結果サマリ（定性的評価）
- **シナリオ別評価**
  - メール本文生成
    - Gemma3: 定型的なビジネスメールであれば、敬体・丁寧語ともにほぼ問題なく実用可能。長文・イレギュラーケースでは言い回しの単調さが残る。
    - gpt-oss-20B: 文脈に応じた表現のバリエーションが増え、挨拶文や締めの一文の自然さが向上。誤用は少なく、人手による軽微な修正でそのまま送信可能なレベル。
  - メール要約
    - Gemma3: 要点抽出は概ね良好だが、長いスレッドでは一部の前提情報が落ちるケースあり。アクションアイテムの抜けが時々発生。
    - gpt-oss-20B: 長文スレッドでも主な論点・依頼事項・期限をバランスよく抽出できる傾向。業務利用時の読み落とし防止に有効。
  - 日本語翻訳
    - Gemma3: 一般的なビジネスメールでは十分自然な日本語を生成。専門用語や社内略語は文脈次第でブレが出ることがある。
    - gpt-oss-20B: 用語の一貫性が高く、ニュアンスの細かい言い回しも比較的忠実に再現可能。レビューコストを下げつつ利用できる。
  - API経由での検証
    - 両モデルとも LM Studio の OpenAI 互換 API 経由で安定して応答。
    - プロンプトテンプレートと JSON 形式（件名・本文分離）の組み合わせで、メール監視システムからの利用を想定したワークフローが実現可能。
- **モデル別所感**
  - Gemma3 系モデル（軽量プロファイル）は、チャット・簡易要約・標準的なメール翻訳・軽めのコード補助など、日常業務の「即時レスポンス」が求められる場面に適する。GPU メモリ使用量も少なく他アプリとの併用もしやすい。
  - gpt-oss-20B モデル（高品質プロファイル）は、長文メールや複雑な依頼内容の整理、コードレビューなど「判断ミスのコストが高い場面」で有効。単発・少数のリクエストであれば本PCスペックでも実用範囲だが、1リクエストあたりの推論時間は比較的長く、連続した大量リクエストや複数ユーザー同時利用では待ち時間増大を前提とした運用設計（キューイングや同時実行数制限など）が必要となる。
- **総評**
  - 本PCクラスであれば、検証した全シナリオ（メール本文生成・要約・翻訳・API 経由連携）について、ローカルのみで「実務で使えるレベル」の生成AI環境を構築可能。ただし、高品質モデル（gpt-oss-20B）は「十分な余裕がある」というより「単発・少数ジョブであればギリギリ実務に耐えうる」レベルであり、多並列処理や複数ユーザー同時利用を行う場合は、キューイングや優先度制御などの運用上の工夫が前提となる。
  - 社外クラウドへ機密情報を送信せずに、メール関連の生産性向上とナレッジ活用を実現できる見込みだが、利用パターンによってはモデル選択（Gemma3 を基本とし、重要処理のみ gpt-oss-20B を利用する等）や運用ポリシーを明確にする必要がある。

## リソース利用と運用上の注意
- **GPU / メモリ**
  - 複数モデルを同時に常駐させると VRAM / RAM を圧迫するため、同時稼働数を制御する運用が望ましい
  - 大規模モデル利用時は、他の GPU 負荷の高い処理（ゲーム、動画編集等）との同時実行は避ける
- **ストレージ**
  - モデルファイルは 1 つあたり数 GB〜数十 GB に達するため、保存先ディスク容量を事前に確保
- **セキュリティ / プライバシー**
  - 推論処理はローカル完結であり、原則として社外への情報送信は発生しない
  - ただし、モデル配布元やアップデート時の通信経路は社内ポリシーに準拠して管理する

## 今後の検討事項
- **RAG（Retrieval-Augmented Generation）との連携**
  - 社内ドキュメントやナレッジベースをローカルでインデックス化し、LM Studio と外部ツールを組み合わせて検索拡張を行う構成の検証
- **複数ユーザー利用**
  - 同一マシン上の LM Studio API Server を、社内ネットワーク内の複数ユーザーから共用する運用の可能性評価
- **モデルラインナップの拡充**
  - 日本語特化モデル、コード特化モデルなどを追加し、用途別に最適なモデル選択ができるようにする
- **監視・ログ**
  - 利用状況（リクエスト数、レイテンシ、エラー率など）を計測し、将来的なスケールアウトや追加GPU導入の判断材料とする

